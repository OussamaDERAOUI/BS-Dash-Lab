{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3XKZwaOnWAJo"
   },
   "source": [
    "# **1. Introduction**\n",
    "\n",
    "**Goals**:\n",
    "\n",
    "\n",
    "*  Learn the usage of textual data\n",
    "*  Get to know popular NLP-frameworks (NLTK, Spacy, Gensim)\n",
    "*  Learn how to preprocess textual data for further analysis (tokenization, stemming/lemmatization, stopword removal, vectorization, similarity computation) \n",
    "* Get to know popular ML framework Scikit-learn\n",
    "* Build model and evaluate it on test set\n",
    "---\n",
    "\n",
    "**Credit**: This tutorial is partly based on [blog post 1](https://curiousily.com/posts/create-dataset-for-sentiment-analysis-by-scraping-google-play-app-reviews-using-python/), [blog post 2](https://curiousily.com/posts/sentiment-analysis-with-bert-and-hugging-face-using-pytorch-and-python/) and the series \"Fundamentals of NLP\" by  Elvis Saravia ( [Twitter](https://twitter.com/omarsar0) | [LinkedIn](https://www.linkedin.com/in/omarsar/)), where the full project is maintained [here](https://github.com/dair-ai/nlp_fundamentals).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSCFR2xLvrBo"
   },
   "source": [
    "Before we start, we can first check the installed library  by following command line:\n",
    "\n",
    "* ``pip freeze``: list all the python libaries & versions\n",
    "* ``pip show <package_name>``: list the selected package_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1724,
     "status": "ok",
     "timestamp": 1648123911402,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "2W9223RYwTgQ",
    "outputId": "9250f9c2-5a49-4ef1-93de-af9cd7627559"
   },
   "outputs": [],
   "source": [
    "#!pip freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8202,
     "status": "ok",
     "timestamp": 1648123928305,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "Qm6Q8gG8xSFH",
    "outputId": "0307cc68-1b9b-4f83-c28c-7dc88d4bff67"
   },
   "outputs": [],
   "source": [
    "#!pip show nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 18,
     "status": "ok",
     "timestamp": 1648123928307,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "jrMMQpfjMFfq"
   },
   "outputs": [],
   "source": [
    "# Initialize random seed to get consistent result\n",
    "import numpy as np\n",
    "import random\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XIFRexvWVwAH"
   },
   "source": [
    "# **2. Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hh1Mk2vuwFFT"
   },
   "source": [
    "<!-- We'll use [The Corpus of Linguistic Acceptability (CoLA)](https://nyu-mll.github.io/CoLA/) dataset for single sentence classification. It's a set of sentences labeled as grammatically correct or incorrect. It was first published in May of 2018, and is one of the tests included in the [General Language Understanding Evaluation (GLUE) Benchmark](https://gluebenchmark.com/). -->\n",
    "We'll use [The Google Play reviews Dataset](https://curiousily.com/posts/create-dataset-for-sentiment-analysis-by-scraping-google-play-app-reviews-using-python/) provided by Venelin Valkov. The dataset can then be used for sentiment analysis classification.\n",
    "\n",
    "You can either download it locally and reupload it in specific folder in your Google Drive, or you can simply run the following code to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 355
    },
    "executionInfo": {
     "elapsed": 1419,
     "status": "ok",
     "timestamp": 1648123966250,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "h4O91aJ6Sj7R",
    "outputId": "9119b7b1-d0c1-46b3-f2da-bf3a69ce927a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15746, 11)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>userName</th>\n",
       "      <th>userImage</th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "      <th>reviewCreatedVersion</th>\n",
       "      <th>at</th>\n",
       "      <th>replyContent</th>\n",
       "      <th>repliedAt</th>\n",
       "      <th>sortOrder</th>\n",
       "      <th>appId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Andrew Thomas</td>\n",
       "      <td>https://lh3.googleusercontent.com/a-/AOh14GiHd...</td>\n",
       "      <td>Update: After getting a response from the deve...</td>\n",
       "      <td>1</td>\n",
       "      <td>21</td>\n",
       "      <td>4.17.0.3</td>\n",
       "      <td>2020-04-05 22:25:57</td>\n",
       "      <td>According to our TOS, and the term you have ag...</td>\n",
       "      <td>2020-04-05 15:10:24</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Craig Haines</td>\n",
       "      <td>https://lh3.googleusercontent.com/-hoe0kwSJgPQ...</td>\n",
       "      <td>Used it for a fair amount of time without any ...</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>4.17.0.3</td>\n",
       "      <td>2020-04-04 13:40:01</td>\n",
       "      <td>It sounds like you logged in with a different ...</td>\n",
       "      <td>2020-04-05 15:11:35</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>steven adkins</td>\n",
       "      <td>https://lh3.googleusercontent.com/a-/AOh14GiXw...</td>\n",
       "      <td>Your app sucks now!!!!! Used to be good but no...</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>4.17.0.3</td>\n",
       "      <td>2020-04-01 16:18:13</td>\n",
       "      <td>This sounds odd! We are not aware of any issue...</td>\n",
       "      <td>2020-04-02 16:05:56</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Lars Panzerbjørn</td>\n",
       "      <td>https://lh3.googleusercontent.com/a-/AOh14Gg-h...</td>\n",
       "      <td>It seems OK, but very basic. Recurring tasks n...</td>\n",
       "      <td>1</td>\n",
       "      <td>192</td>\n",
       "      <td>4.17.0.2</td>\n",
       "      <td>2020-03-12 08:17:34</td>\n",
       "      <td>We do offer this option as part of the Advance...</td>\n",
       "      <td>2020-03-15 06:20:13</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Scott Prewitt</td>\n",
       "      <td>https://lh3.googleusercontent.com/-K-X1-YsVd6U...</td>\n",
       "      <td>Absolutely worthless. This app runs a prohibit...</td>\n",
       "      <td>1</td>\n",
       "      <td>42</td>\n",
       "      <td>4.17.0.2</td>\n",
       "      <td>2020-03-14 17:41:01</td>\n",
       "      <td>We're sorry you feel this way! 90% of the app ...</td>\n",
       "      <td>2020-03-15 23:45:51</td>\n",
       "      <td>most_relevant</td>\n",
       "      <td>com.anydo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           userName                                          userImage  \\\n",
       "0     Andrew Thomas  https://lh3.googleusercontent.com/a-/AOh14GiHd...   \n",
       "1      Craig Haines  https://lh3.googleusercontent.com/-hoe0kwSJgPQ...   \n",
       "2     steven adkins  https://lh3.googleusercontent.com/a-/AOh14GiXw...   \n",
       "3  Lars Panzerbjørn  https://lh3.googleusercontent.com/a-/AOh14Gg-h...   \n",
       "4     Scott Prewitt  https://lh3.googleusercontent.com/-K-X1-YsVd6U...   \n",
       "\n",
       "                                             content  score  thumbsUpCount  \\\n",
       "0  Update: After getting a response from the deve...      1             21   \n",
       "1  Used it for a fair amount of time without any ...      1             11   \n",
       "2  Your app sucks now!!!!! Used to be good but no...      1             17   \n",
       "3  It seems OK, but very basic. Recurring tasks n...      1            192   \n",
       "4  Absolutely worthless. This app runs a prohibit...      1             42   \n",
       "\n",
       "  reviewCreatedVersion                   at  \\\n",
       "0             4.17.0.3  2020-04-05 22:25:57   \n",
       "1             4.17.0.3  2020-04-04 13:40:01   \n",
       "2             4.17.0.3  2020-04-01 16:18:13   \n",
       "3             4.17.0.2  2020-03-12 08:17:34   \n",
       "4             4.17.0.2  2020-03-14 17:41:01   \n",
       "\n",
       "                                        replyContent            repliedAt  \\\n",
       "0  According to our TOS, and the term you have ag...  2020-04-05 15:10:24   \n",
       "1  It sounds like you logged in with a different ...  2020-04-05 15:11:35   \n",
       "2  This sounds odd! We are not aware of any issue...  2020-04-02 16:05:56   \n",
       "3  We do offer this option as part of the Advance...  2020-03-15 06:20:13   \n",
       "4  We're sorry you feel this way! 90% of the app ...  2020-03-15 23:45:51   \n",
       "\n",
       "       sortOrder      appId  \n",
       "0  most_relevant  com.anydo  \n",
       "1  most_relevant  com.anydo  \n",
       "2  most_relevant  com.anydo  \n",
       "3  most_relevant  com.anydo  \n",
       "4  most_relevant  com.anydo  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the data via pandas\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"reviews.csv\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 236,
     "status": "ok",
     "timestamp": 1648123973277,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "8wnXWebjTecS",
    "outputId": "b2350e44-fae6-41a6-85fc-a45b22f241fa"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>score</th>\n",
       "      <th>thumbsUpCount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>15746.000000</td>\n",
       "      <td>15746.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.062365</td>\n",
       "      <td>3.992570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.310503</td>\n",
       "      <td>17.058478</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>448.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              score  thumbsUpCount\n",
       "count  15746.000000   15746.000000\n",
       "mean       3.062365       3.992570\n",
       "std        1.310503      17.058478\n",
       "min        1.000000       0.000000\n",
       "25%        2.000000       0.000000\n",
       "50%        3.000000       0.000000\n",
       "75%        4.000000       1.000000\n",
       "max        5.000000     448.000000"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print the data infos & check for missing values\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 234,
     "status": "ok",
     "timestamp": 1648123978199,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "nbVu7OicV2bB",
    "outputId": "2e91d65c-253c-433a-a2f2-f479f2a6b0ed"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Update: After getting a response from the deve...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Used it for a fair amount of time without any ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Your app sucks now!!!!! Used to be good but no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>It seems OK, but very basic. Recurring tasks n...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Absolutely worthless. This app runs a prohibit...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  score\n",
       "0  Update: After getting a response from the deve...      1\n",
       "1  Used it for a fair amount of time without any ...      1\n",
       "2  Your app sucks now!!!!! Used to be good but no...      1\n",
       "3  It seems OK, but very basic. Recurring tasks n...      1\n",
       "4  Absolutely worthless. This app runs a prohibit...      1"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actually we only need the content(i.e. review) & the sentiment score for classification\n",
    "df = df[['content', 'score']]\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 334
    },
    "executionInfo": {
     "elapsed": 783,
     "status": "ok",
     "timestamp": 1648123983896,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "1aRee_tWWhGq",
    "outputId": "dbbae3c3-f70c-4495-e766-216f7b8bb41b"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Usuario\\Desktop\\QFM\\S2\\webmining\\2-Part 2 Lab SA\\2-Sentiment analysis Google Reviews Dataset.ipynb Cell 11\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/QFM/S2/webmining/2-Part%202%20Lab%20SA/2-Sentiment%20analysis%20Google%20Reviews%20Dataset.ipynb#X13sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# We can plot the score & check for the distribution\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/QFM/S2/webmining/2-Part%202%20Lab%20SA/2-Sentiment%20analysis%20Google%20Reviews%20Dataset.ipynb#X13sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mseaborn\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msns\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/QFM/S2/webmining/2-Part%202%20Lab%20SA/2-Sentiment%20analysis%20Google%20Reviews%20Dataset.ipynb#X13sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpyplot\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mplt\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/QFM/S2/webmining/2-Part%202%20Lab%20SA/2-Sentiment%20analysis%20Google%20Reviews%20Dataset.ipynb#X13sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m sns\u001b[39m.\u001b[39mcountplot(x\u001b[39m=\u001b[39mdf\u001b[39m.\u001b[39mscore)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "# We can plot the score & check for the distribution\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "sns.countplot(x=df.score)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "executionInfo": {
     "elapsed": 744,
     "status": "ok",
     "timestamp": 1648123988459,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "w8QhZTegZR-l",
    "outputId": "85e59727-8e59-4094-888b-10ffe9d9bead"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0, 0, 'negative'), Text(1, 0, 'neutral'), Text(2, 0, 'positive')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Since it's highly imbalanced, we can convert the score into 3 categories: negative(<3), neutral(=3), positive(>3)\n",
    "# And replot it\n",
    "def to_sentiment(score):\n",
    "    score = int(score)\n",
    "    if score <= 2:\n",
    "        return 0\n",
    "    elif score == 3:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2\n",
    "df['sentiment'] = df.score.apply(to_sentiment)\n",
    "\n",
    "# Plot the score distribution after conversion\n",
    "ax = sns.countplot(x=df.sentiment)\n",
    "class_names = ['negative', 'neutral', 'positive']\n",
    "plt.xlabel('review sentiment')\n",
    "ax.set_xticklabels(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCgy1XjEcEQT"
   },
   "source": [
    "# **3. Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 272,
     "status": "ok",
     "timestamp": 1648123994745,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "19FMY8c7b_1C",
    "outputId": "ebb78b1c-17cf-4450-f985-de96f7870d90"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Update: After getting a response from the developer I would change my rating to 0 stars if possible. These guys hide behind confusing and opaque terms and refuse to budge at all. I'm so annoyed that my money has been lost to them! Really terrible customer experience. Original: Be very careful when signing up for a free trial of this app. If you happen to go over they automatically charge you for a full years subscription and refuse to refund. Terrible customer experience and the app is just OK.\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Before preprocessing, we can first check the review content\n",
    "df.content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "executionInfo": {
     "elapsed": 472,
     "status": "ok",
     "timestamp": 1648123997897,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "vgEB9R0hdi_f",
    "outputId": "aa9bee74-ebce-45de-a93d-5bb1db604dca"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"It seems OK, but very basic. Recurring tasks need some work to be actually useful. For example, it would be nice to be able to set a task to be recurring on the first of every month, without only being able to set that up on the first of the month. Edit; I also just noticed that there is no dark theme. Both may be available as paid for options, but I'll never know, since they are basic options and without them, I have no reason to try this app, and thus will never pay for actual premium options.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.content[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_BPXrTKHe086"
   },
   "source": [
    "## **3.1 Tokenization**\n",
    "<div>\n",
    "<img src=https://drive.google.com/uc?export=view&id=1h0ZNzohff1nUWMerrW50eDxY99ArRJTK width=\"800\">\n",
    "</div>\n",
    "\n",
    "With any typical NLP task, one of the first steps is to tokenize your pieces of text into its individual words/tokens (process demonstrated in the figure above), the result of which is used to create so-called vocabularies that will be used in the langauge model you plan to build. This is actually one of the techniques that we will use the most throughout this series but here we stick to the basics.\n",
    "\n",
    "Below I am showing you an example of a simple tokenizer without following any standards. All it does is extract tokens based on a white space seperator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 222,
     "status": "ok",
     "timestamp": 1648124018952,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "9YB4Yd-9erXB",
    "outputId": "326cf9b1-2852-4327-c62d-8da6fea2c87b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 0: Update:\n",
      "Token 1: After\n",
      "Token 2: getting\n",
      "Token 3: a\n",
      "Token 4: response\n",
      "Token 5: from\n",
      "Token 6: the\n",
      "Token 7: developer\n",
      "Token 8: I\n",
      "Token 9: would\n",
      "Token 10: change\n",
      "Token 11: my\n",
      "Token 12: rating\n",
      "Token 13: to\n",
      "Token 14: 0\n",
      "Token 15: stars\n",
      "Token 16: if\n",
      "Token 17: possible.\n",
      "Token 18: These\n",
      "Token 19: guys\n",
      "Token 20: hide\n",
      "Token 21: behind\n",
      "Token 22: confusing\n",
      "Token 23: and\n",
      "Token 24: opaque\n",
      "Token 25: terms\n",
      "Token 26: and\n",
      "Token 27: refuse\n",
      "Token 28: to\n",
      "Token 29: budge\n",
      "Token 30: at\n",
      "Token 31: all.\n",
      "Token 32: I'm\n",
      "Token 33: so\n",
      "Token 34: annoyed\n",
      "Token 35: that\n",
      "Token 36: my\n",
      "Token 37: money\n",
      "Token 38: has\n",
      "Token 39: been\n",
      "Token 40: lost\n",
      "Token 41: to\n",
      "Token 42: them!\n",
      "Token 43: Really\n",
      "Token 44: terrible\n",
      "Token 45: customer\n",
      "Token 46: experience.\n",
      "Token 47: Original:\n",
      "Token 48: Be\n",
      "Token 49: very\n",
      "Token 50: careful\n",
      "Token 51: when\n",
      "Token 52: signing\n",
      "Token 53: up\n",
      "Token 54: for\n",
      "Token 55: a\n",
      "Token 56: free\n",
      "Token 57: trial\n",
      "Token 58: of\n",
      "Token 59: this\n",
      "Token 60: app.\n",
      "Token 61: If\n",
      "Token 62: you\n",
      "Token 63: happen\n",
      "Token 64: to\n",
      "Token 65: go\n",
      "Token 66: over\n",
      "Token 67: they\n",
      "Token 68: automatically\n",
      "Token 69: charge\n",
      "Token 70: you\n",
      "Token 71: for\n",
      "Token 72: a\n",
      "Token 73: full\n",
      "Token 74: years\n",
      "Token 75: subscription\n",
      "Token 76: and\n",
      "Token 77: refuse\n",
      "Token 78: to\n",
      "Token 79: refund.\n",
      "Token 80: Terrible\n",
      "Token 81: customer\n",
      "Token 82: experience\n",
      "Token 83: and\n",
      "Token 84: the\n",
      "Token 85: app\n",
      "Token 86: is\n",
      "Token 87: just\n",
      "Token 88: OK.\n"
     ]
    }
   ],
   "source": [
    "# You can simply use split to tokenize the text by whitespace\n",
    "doc = df.content[0]\n",
    "for i, w in enumerate(doc.split(\" \")):\n",
    "    print(\"Token \" + str(i) + \": \" + w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pfwo0Oyc8Elz"
   },
   "source": [
    "**Question:**\n",
    "\n",
    "**(a) What are the problems here with only the split function for tokenization?**\n",
    "\n",
    "White space tokenization, where words are simply split by spaces, is a common technique for natural language processing (NLP). However, it can lead to several problems:\n",
    "\n",
    "* **Punctuation**: White space tokenization does not take into account punctuation marks, which can change the meaning of a sentence. For example, \"Let's eat, grandma!\" and \"Let's eat grandma!\" have completely different meanings.\n",
    "\n",
    "* **Contractions**: White space tokenization can also create problems with contractions, such as \"can't\" and \"won't\". These are typically treated as a single word, but can also be split into two words (\"can\" and \"not\").\n",
    "\n",
    "* **Hyphenated words**: White space tokenization can also have difficulty with hyphenated words, which can be treated as separate words or as a single unit depending on the context.\n",
    "\n",
    "* **Multi-word expressions**: White space tokenization also struggles with multi-word expressions, such as \"New York\" or \"United States\". These can be split into separate words or treated as a single unit depending on the context.\n",
    "\n",
    "* **Ambiguity**: White space tokenization can be ambiguous, especially when dealing with languages that do not use spaces to separate words, such as Chinese and Japanese. In these cases, the meaning of the text can change depending on how it is segmented.\n",
    "\n",
    "**(b) How can we improve the tokenization?**\n",
    "\n",
    "Tokenization can come in different forms. For instance, more recently a lot of state-of-the-art NLP models such as [BERT](https://arxiv.org/pdf/1810.04805.pdf) make use of `subword` tokens in which frequent combinations of characters also form part of the vocabulary. This helps to deal with the so-called out of vocabulary (OOV) problem. We will discuss this in upcoming chapters, but if you are interested in reading more about this now, check this [paper](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/37842.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOUlECPwYYj2"
   },
   "source": [
    "### **3.1.1 spaCy**\n",
    "\n",
    "To demonstrate how you can achieve more reliable tokenization, we are going to use [spaCy](https://spacy.io/), which is an impressive and robust Python library for natural language processing. It comes with pretrained pipelines and currently supports tokenization and training for 60+ languages. It features state-of-the-art speed and neural network models for tagging, parsing, named entity recognition, text classification and more.\n",
    "\n",
    "In particular, we are going to use the built-in tokenizer found [here](https://spacy.io/usage/linguistic-features#tokenization). You can also find the useful models for different languages [here](https://spacy.io/usage/models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3076,
     "status": "ok",
     "timestamp": 1648124087535,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "Hjh2Wmjf8YMh",
    "outputId": "d89048a7-5e84-430d-e821-d817aaab4afb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Colab has already spacy library installed\n",
    "#!pip show spacy\n",
    "#If you don't have it in your local machine, you can simply download the libraries\n",
    "#!pip install -U spacy\n",
    "#!pip install -U spacy-lookups-data\n",
    "#!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install --upgrade --user tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade google-api-core\n",
    "# !pip install --upgrade google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1809,
     "status": "ok",
     "timestamp": 1648126796951,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "xpfzJRYL8dVB",
    "outputId": "2f850e4f-ccd9-4ed9-afd9-fc03486d0581",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 0: Used (VERB)\n",
      "Token 1: it (PRON)\n",
      "Token 2: for (ADP)\n",
      "Token 3: a (DET)\n",
      "Token 4: fair (ADJ)\n",
      "Token 5: amount (NOUN)\n",
      "Token 6: of (ADP)\n",
      "Token 7: time (NOUN)\n",
      "Token 8: without (ADP)\n",
      "Token 9: any (DET)\n",
      "Token 10: problems (NOUN)\n",
      "Token 11: . (PUNCT)\n",
      "Token 12: Suddenly (ADV)\n",
      "Token 13: then (ADV)\n",
      "Token 14: asked (VERB)\n",
      "Token 15: me (PRON)\n",
      "Token 16: to (PART)\n",
      "Token 17: create (VERB)\n",
      "Token 18: an (DET)\n",
      "Token 19: account (NOUN)\n",
      "Token 20: or (CCONJ)\n",
      "Token 21: log (VERB)\n",
      "Token 22: using (VERB)\n",
      "Token 23: Google (PROPN)\n",
      "Token 24: or (CCONJ)\n",
      "Token 25: FB (PROPN)\n",
      "Token 26: . (PUNCT)\n",
      "Token 27: I (PRON)\n",
      "Token 28: used (VERB)\n",
      "Token 29: my (PRON)\n",
      "Token 30: Google (PROPN)\n",
      "Token 31: one (NOUN)\n",
      "Token 32: only (ADV)\n",
      "Token 33: to (PART)\n",
      "Token 34: discover (VERB)\n",
      "Token 35: everything (PRON)\n",
      "Token 36: was (AUX)\n",
      "Token 37: gone (VERB)\n",
      "Token 38: ! (PUNCT)\n"
     ]
    }
   ],
   "source": [
    "## Import the libraries\n",
    "import spacy\n",
    "## Load the language model for the language you need\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "## Do tokenization\n",
    "doc = nlp(df.content[1])\n",
    "for i, token in enumerate(doc):\n",
    "    #print(\"Token {}: {}\".format(str(i), token.text))\n",
    "    print(\"Token {}: {} ({})\".format(str(i), token.text, token.pos_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X55d7q-dAleh"
   },
   "source": [
    "All the code does is tokenize the text based on a pre-built language model.\n",
    "\n",
    "Try putting different running text into the `nlp()` part of the code above. The tokenizer is quite robust and it includes a series of built-in rules that deal with exceptions and special cases such as those tokens that contain puctuations like \"`\" and \".\", \"-\", etc. You can even add your own rules, find out how [here](https://spacy.io/usage/linguistic-features#special-cases).\n",
    "\n",
    "Other tools you can use for tokenization are the [Keras Tokenizer API](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer), [Hugging Face Tokenizer](https://github.com/huggingface/tokenizers) and [NLTK](https://www.nltk.org/), which we are going to use for our data.  \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qseAQoTkZBhF"
   },
   "source": [
    "### **3.1.2 NLTK**\n",
    "\n",
    "[NLTK](https://www.nltk.org/) stands for **Natural Language Toolkit**. It is a leading platform for building Python programs to work with human language data. It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet, along with a suite of text processing libraries for classification, tokenization, stemming, tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1527,
     "status": "ok",
     "timestamp": 1648124156035,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "bWr3glXzDE5l",
    "outputId": "993ae205-cf29-4114-a7fd-eaa558edf3a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's tokenize the data; we will do so using the nltk library \n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8445,
     "status": "ok",
     "timestamp": 1648124166846,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "IXf_NCOsDULD",
    "outputId": "26cb386c-1084-495b-ed5c-523e1d509320"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Update', ':', 'After', 'getting', 'a', 'response', 'from', 'the', 'developer', 'I', 'would', 'change', 'my', 'rating', 'to', '0', 'stars', 'if', 'possible', '.', 'These', 'guys', 'hide', 'behind', 'confusing', 'and', 'opaque', 'terms', 'and', 'refuse', 'to', 'budge', 'at', 'all', '.', 'I', \"'m\", 'so', 'annoyed', 'that', 'my', 'money', 'has', 'been', 'lost', 'to', 'them', '!', 'Really', 'terrible', 'customer', 'experience', '.', 'Original', ':', 'Be', 'very', 'careful', 'when', 'signing', 'up', 'for', 'a', 'free', 'trial', 'of', 'this', 'app', '.', 'If', 'you', 'happen', 'to', 'go', 'over', 'they', 'automatically', 'charge', 'you', 'for', 'a', 'full', 'years', 'subscription', 'and', 'refuse', 'to', 'refund', '.', 'Terrible', 'customer', 'experience', 'and', 'the', 'app', 'is', 'just', 'OK', '.']\n",
      "CPU times: total: 3.09 s\n",
      "Wall time: 7.47 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "all_texts = list(df.content)\n",
    "all_texts_tokenized = [nltk.word_tokenize(t) for t in all_texts]\n",
    "print(all_texts_tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JdUNF24ZjuN"
   },
   "source": [
    "## **3.2 Emoji detection**\n",
    "\n",
    "Emoji lexicons are one of the key components for detecting emotions. \n",
    "Since emoji characters are a subset of a large Unicode character set, \n",
    "we will use the python package [emoji](https://github.com/carpedm20/emoji/) to demojize the emoji lexicons. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5028,
     "status": "ok",
     "timestamp": 1648124191914,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "G2bgj-ca1NBG",
    "outputId": "3a5f88c8-0e53-4fe6-ebfc-e447ff60f763"
   },
   "outputs": [],
   "source": [
    "#!pip install emoji\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 378,
     "status": "ok",
     "timestamp": 1648124198389,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "57_yRv_4bhX4",
    "outputId": "1de5d8c4-25c8-4324-cd9d-18b0e4b031a6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Supported languages: ['en', 'es', 'ja', 'ko', 'pt', 'it', 'fr', 'de', 'fa', 'id', 'zh']\n",
      "First five emoticons with their english translation: [('🥇', ':1st_place_medal:'), ('🥈', ':2nd_place_medal:'), ('🥉', ':3rd_place_medal:'), ('🆎', ':AB_button_(blood_type):'), ('🏧', ':ATM_sign:')]\n"
     ]
    }
   ],
   "source": [
    "# You can check a list of emoticons by emoji.UNICODE_EMOJI\n",
    "print('Supported languages: {}'.format(list(emoji.LANGUAGES)))\n",
    "print('First five emoticons with their english translation: {}'.format([(emoticon[0], emoticon[1]['en']) \n",
    "                                                                    for emoticon in list(emoji.EMOJI_DATA.items())[0:5]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 215,
     "status": "ok",
     "timestamp": 1648124207805,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "5SlFCQrva-Z2",
    "outputId": "7b5ea550-b063-499b-8c52-dbf75e25759c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This was originally a 5 star app, but now you can only use it by allowing it access to either your emails or Facebook! 😡😡😡 I am definitely not happy to allow this so have deleted the app. Would give it zero stars if I could.\n"
     ]
    }
   ],
   "source": [
    "# We will first check one of the reviews which contain the emojis\n",
    "print(all_texts[15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "executionInfo": {
     "elapsed": 214,
     "status": "ok",
     "timestamp": 1648124210859,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "iLKPdroBa74B",
    "outputId": "05380498-b3b8-4aad-dc85-9052f038d55f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This was originally a 5 star app, but now you can only use it by allowing it access to either your emails or Facebook! :enraged_face::enraged_face::enraged_face: I am definitely not happy to allow this so have deleted the app. Would give it zero stars if I could.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Then we will use the demojize function to convert unicode set to text\n",
    "emoji.demojize(all_texts[15], language='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 16683,
     "status": "ok",
     "timestamp": 1648124235676,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "9_JRXbV37u5Y",
    "outputId": "8e9f5115-c044-4bc5-e779-85c2ef59bc85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'was', 'originally', 'a', '5', 'star', 'app', ',', 'but', 'now', 'you', 'can', 'only', 'use', 'it', 'by', 'allowing', 'it', 'access', 'to', 'either', 'your', 'emails', 'or', 'facebook', '!', ':', 'enraged_face', ':', ':enraged_face', ':', ':enraged_face', ':', 'i', 'am', 'definitely', 'not', 'happy', 'to', 'allow', 'this', 'so', 'have', 'deleted', 'the', 'app', '.', 'would', 'give', 'it', 'zero', 'stars', 'if', 'i', 'could', '.']\n",
      "this was originally a 5 star app , but now you can only use it by allowing it access to either your emails or facebook ! : enraged_face : :enraged_face : :enraged_face : i am definitely not happy to allow this so have deleted the app . would give it zero stars if i could .\n"
     ]
    }
   ],
   "source": [
    "# So to combine 3.1 tokenization and 3.2 Emoji detection together with lower case, we will do the following:\n",
    "all_texts = list(df.content)\n",
    "all_texts_preprocessed_tokenized = [nltk.word_tokenize(emoji.demojize(t.lower(), language='en')) for t in all_texts]\n",
    "print(all_texts_preprocessed_tokenized[15])\n",
    "all_texts_preprocessed = [' '.join(text) for text in all_texts_preprocessed_tokenized]\n",
    "print(all_texts_preprocessed[15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9uveJKJeLHY"
   },
   "source": [
    "To fit your needs, there are also some common text preprocessing steps:\n",
    "* **lemmatization** (playing -> play)\n",
    "* **stemming** (troubled -> troubl)\n",
    "* **stop words removal** (a, the, is, are, ...)\n",
    "* **punctuation removal**\n",
    "* **abbreviation conversion** (b4 -> before)\n",
    "* **lower case** (The -> the) \n",
    "* **digits conversion** (1549 -> 0000)\n",
    "* **[regular expression](https://regexone.com/)**\n",
    "* **sentence segmentation**\n",
    "\n",
    "and more to explore.\n",
    "\n",
    "**We leave the above as exercises for you to try out.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rejv-8B3yLb3"
   },
   "outputs": [],
   "source": [
    "### Exercise: try out different preprocessing steps\n",
    "### Your turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2627,
     "status": "ok",
     "timestamp": 1648124247449,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "4DbwemmV_7eE",
    "outputId": "5f419b11-6a81-495e-f9b9-733b34b62db4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "am : be\n",
      "are : be\n",
      "is : be\n",
      "was : be\n",
      "survival : survival\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"am :\", lemmatizer.lemmatize(\"am\", pos=\"v\"))  \n",
    "print(\"are :\", lemmatizer.lemmatize(\"are\", pos=\"v\"))\n",
    "print(\"is :\", lemmatizer.lemmatize(\"is\", pos=\"v\"))\n",
    "print(\"was :\", lemmatizer.lemmatize(\"was\", pos=\"v\"))\n",
    "print(\"survival :\", lemmatizer.lemmatize(\"survival\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 262,
     "status": "ok",
     "timestamp": 1648124251537,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "Liy4gAoEAez0",
    "outputId": "b9ca21c8-fd3f-4515-ff80-3b47a9dbdc69"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "playing : play\n",
      "played : play\n",
      "plays : play\n",
      "survival : surviv\n"
     ]
    }
   ],
   "source": [
    "# Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(\"playing :\", stemmer.stem(\"playing\"))\n",
    "print(\"played :\", stemmer.stem(\"played\"))\n",
    "print(\"plays :\", stemmer.stem(\"plays\"))\n",
    "print(\"survival :\", stemmer.stem(\"survival\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1648124255481,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "vDYQk4oWDdrn",
    "outputId": "47a659a5-6169-4850-d090-bce2def74028"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n",
      "Welcome to web mining course\n"
     ]
    }
   ],
   "source": [
    "# Punctuations removal\n",
    "import string\n",
    "print(string.punctuation) \n",
    "doc = \"Welcome to web mining course!!!\"\n",
    "print(doc.translate(str.maketrans('', '', string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 613,
     "status": "ok",
     "timestamp": 1648124263893,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "YPp9vg5CD6En",
    "outputId": "f45ff4d2-00db-4c7a-b2cf-d07e4d117ff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I love coding and programming.\n",
      "I also love sleeping!\n"
     ]
    }
   ],
   "source": [
    "# Sentence segmentation\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"I love coding and programming. I also love sleeping!\")\n",
    "for sent in doc.sents:\n",
    "    print(sent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-K7yZU95lc1X"
   },
   "source": [
    "# **4. Vectorization**\n",
    "\n",
    "Many machine learning models require numeric representations in form of feature vectors as input. Transforming text to vectors is called **vectorization**.\n",
    "\n",
    "When you are working with textual information, it is imperative to clean your data so as to be able to train more accurate machine learning (ML) models. \n",
    "\n",
    "One of the reasons why transformations like lemmatization and stemming are useful is for normalizing the text before you feed the output to an ML algorithm. For instance, if you are building a sentiment analysis model how can you tell the model that \"smiling\" and \"smile\" refer to the same concept? You may require stemming if you are using [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) features combined with a machine learning algorithm such as [Naive Bayes classifier](https://en.wikipedia.org/wiki/Naive_Bayes_classifier). As you may suspect already, this also requires a really good tokenizer to come up with the features, especially when you work on noisy pieces of text that could be generated from users in a social media site.\n",
    "\n",
    "With a wide variety of NLP tasks, one of the first big steps in the NLP pipeline is to create a vocabulary that will eventually be used to determine the inputs for the model representing the features. In modern NLP techniques such as pretrained language models, you need to process a text corpus. This requires a proper and more sophisticated sentence segmentation and tokenization as we discussed before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uc0a7Ipr8Rv-"
   },
   "source": [
    "## **4.1 Tf-Idf Vectorization (sparse)**\n",
    "\n",
    "A simple and powerful technique, which is mostly used for traditional machine learning models is the so-called Tf-Idf vectorization.\n",
    "\n",
    "Tf-idf is a product of two values:\n",
    "* tf: term frequency (often log-transformed)\n",
    "* idf: inverse document frequency\n",
    "\n",
    "\n",
    "\n",
    "Term frequency (tf) measures how frequently the word t\n",
    "appears in the document d\n",
    "\n",
    "$$\\text{tf}_{td}= \\begin{cases} 1 + \\text{log}_{10} \\text{count}(t,d), & \\text{if  count}(t,d) \n",
    "> 0 \\\\ 0, &\\text{otherwise} \\end{cases}$$\n",
    "\n",
    "Inverse document frequency (idf) is inversely proportional\n",
    "to the number of documents in a corpus that contain t\n",
    "(frequently appeared words among documents will have lower idf values)\n",
    "\n",
    "$$\\text{idf}_t= \\text{log}_{10} \\frac{N}{\\text{df}_t}$$\n",
    "\n",
    "So, the tf-idf value for word t in document d is\n",
    "\n",
    "$$\\text{tfidf}_{td} = \\text{tf}_{td}*\\text{idf}_t$$ .\n",
    "\n",
    "So tfidf values will promote document-specific words, and penalize non-specific words (e.g. a, the, ...)\n",
    "\n",
    "[Gensim](https://radimrehurek.com/gensim/index.html) provides an implementation of a tfidf vectorizer.\n",
    "Let's see how we can use this with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 10962\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import TfidfModel\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Tokenize content by whitespace\n",
    "tokenized_content = [list(simple_preprocess(review)) for review in all_texts_preprocessed]\n",
    "dct = Dictionary(tokenized_content)  # fit dictionary\n",
    "corpus = [dct.doc2bow(line) for line in tokenized_content]\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "\n",
    "print('Number of features: {}'.format(len(dct)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mc3IrdTq8as8"
   },
   "source": [
    "Let's inspect what numbers we get ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 0:\n",
      "update : after getting a response from the developer i would change my rating to 0 stars if possible . these guys hide behind confusing and opaque terms and refuse to budge at all . i 'm so annoyed that my money has been lost to them ! really terrible customer experience . original : be very careful when signing up for a free trial of this app . if you happen to go over they automatically charge you for a full years subscription and refuse to refund . terrible customer experience and the app is just ok .\n",
      "\n",
      "refuse:0.35477385861173566\n",
      "terrible:0.2651223479693971\n",
      "customer:0.26227242053190264\n",
      "budge:0.2287673900887692\n",
      "opaque:0.2287673900887692\n",
      "experience:0.22369946470383262\n",
      "careful:0.18307713790169963\n",
      "signing:0.17574118312330317\n",
      "annoyed:0.16648701490959536\n",
      "hide:0.15237557220564932\n",
      "terms:0.15174590526888254\n",
      "charge:0.15113141251600623\n",
      "happen:0.1482631997248816\n",
      "refund:0.1381052491417779\n",
      "original:0.13738688571463006\n",
      "trial:0.1347001698838273\n",
      "behind:0.12825620546755281\n",
      "rating:0.11971148339280806\n",
      "response:0.11867051374456994\n",
      "guys:0.11718464404354549\n",
      "possible:0.11718464404354549\n",
      "confusing:0.11702476756200528\n",
      "developer:0.11416333764504634\n",
      "lost:0.11416333764504634\n",
      "automatically:0.10984868926514899\n",
      "these:0.10754981688448007\n",
      "ok:0.10711390515380675\n",
      "money:0.10554255296858442\n",
      "if:0.09941095928315279\n",
      "getting:0.09787853547782858\n",
      "subscription:0.09691893513723496\n",
      "full:0.09585479400600727\n",
      "over:0.09483328698826506\n",
      "years:0.09410936129361223\n",
      "stars:0.08721519222807696\n",
      "go:0.08672478835887651\n",
      "change:0.08383488248666875\n",
      "them:0.08147551666178537\n",
      "you:0.08072687828848918\n",
      "and:0.0794626621697783\n",
      "free:0.07659669223163372\n",
      "update:0.07653147425420344\n",
      "been:0.07364040528978014\n",
      "they:0.0719528956520631\n",
      "after:0.07138769623941503\n",
      "my:0.06992374684006733\n",
      "to:0.06963044931352676\n",
      "from:0.06460333713086579\n",
      "has:0.06368178366231295\n",
      "at:0.06258420973938882\n",
      "up:0.060853792039267385\n",
      "really:0.06021038391112653\n",
      "very:0.05682248229968295\n",
      "all:0.05516358344343557\n",
      "when:0.05516358344343557\n",
      "for:0.05461781083636699\n",
      "just:0.05437292327691484\n",
      "would:0.05287442913273004\n",
      "so:0.04827890879627113\n",
      "be:0.0436852982211507\n",
      "that:0.037935876489323075\n",
      "app:0.036683066900718896\n",
      "of:0.032833072634282694\n",
      "this:0.029565548417062174\n",
      "the:0.029506024041839093\n",
      "is:0.026154016185603583\n"
     ]
    }
   ],
   "source": [
    "doc = 0 #Check for our first review in our dataset\n",
    "print('Text 0:\\n{}\\n'.format(all_texts_preprocessed[0]))\n",
    "\n",
    "# Inspect tf-idf scores of document sorted by the similarity score\n",
    "for w,s in sorted(tfidf_model[corpus[doc]], reverse=True, key=lambda x: x[1]):\n",
    "    print('{}:{}'.format(dct[w], s))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "klLuSON2pUJy"
   },
   "source": [
    "**You can see that there are actually some missing tokens after applying the tfidf-vectorizer (e.g. tokens with the length less than 2). Why is this the case? How can you modify it?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 846,
     "status": "ok",
     "timestamp": 1648124384782,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "5ZIoifyLx6kN",
    "outputId": "7d1ccb2c-ccc6-4454-9c1a-a36d1fc9dea0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 11042\n"
     ]
    }
   ],
   "source": [
    "# Tokenize content by whitespace and decrease min length to 1\n",
    "tokenized_content = [list(simple_preprocess(review, min_len=1)) for review in all_texts_preprocessed]\n",
    "dct = Dictionary(tokenized_content)  # fit dictionary\n",
    "corpus = [dct.doc2bow(line) for line in tokenized_content]\n",
    "tfidf_model = TfidfModel(corpus)\n",
    "\n",
    "print('Number of features: {}'.format(len(dct)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1648124405880,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "T6Si07iQHe0j",
    "outputId": "2c842f8b-9b19-494e-c9d2-d47a20e307af",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text 0:\n",
      "update : after getting a response from the developer i would change my rating to 0 stars if possible . these guys hide behind confusing and opaque terms and refuse to budge at all . i 'm so annoyed that my money has been lost to them ! really terrible customer experience . original : be very careful when signing up for a free trial of this app . if you happen to go over they automatically charge you for a full years subscription and refuse to refund . terrible customer experience and the app is just ok .\n",
      "\n",
      "refuse:0.353065815711946\n",
      "terrible:0.2638459282641892\n",
      "customer:0.2610097216750784\n",
      "budge:0.22766599970483972\n",
      "opaque:0.22766599970483972\n",
      "experience:0.2226224736203597\n",
      "careful:0.1821957211966176\n",
      "signing:0.1748950850449132\n",
      "annoyed:0.1656854706108236\n",
      "hide:0.15164196681762643\n",
      "terms:0.1510153313842088\n",
      "charge:0.15040379707925092\n",
      "happen:0.1475493931705286\n",
      "refund:0.13744034758690193\n",
      "original:0.13672544268839545\n",
      "trial:0.13405166193098425\n",
      "behind:0.12763872169363544\n",
      "rating:0.11913513780175326\n",
      "response:0.11809917985540189\n",
      "guys:0.11662046380770122\n",
      "possible:0.11662046380770122\n",
      "confusing:0.11646135704434206\n",
      "developer:0.11361370335394061\n",
      "lost:0.11361370335394061\n",
      "automatically:0.10931982765599679\n",
      "these:0.107032023093748\n",
      "ok:0.10659821004063628\n",
      "money:0.10503442306034068\n",
      "if:0.09893234965890003\n",
      "getting:0.09740730363955588\n",
      "subscription:0.0964523232519509\n",
      "full:0.09539330538068007\n",
      "over:0.09437671636285917\n",
      "years:0.09365627597613621\n",
      "stars:0.08679529857970648\n",
      "go:0.08630725573803522\n",
      "change:0.08343126318859989\n",
      "them:0.08108325642512246\n",
      "you:0.0803382223378439\n",
      "and:0.07908009273118276\n",
      "free:0.07622792087732491\n",
      "update:0.07616301688893465\n",
      "been:0.0732858668469393\n",
      "they:0.07160648164901698\n",
      "after:0.07104400336370201\n",
      "my:0.06958710208336452\n",
      "to:0.06929521662466821\n",
      "m:0.06728387770381582\n",
      "from:0.06429230724912399\n",
      "a:0.06420597776077985\n",
      "has:0.06337519055859334\n",
      "at:0.06228290085630875\n",
      "up:0.06056081417493241\n",
      "really:0.05992050370649297\n",
      "very:0.056548912996070275\n",
      "all:0.054898000834287836\n",
      "when:0.054898000834287836\n",
      "for:0.05435485582506441\n",
      "just:0.054111147265831744\n",
      "would:0.05261986755478857\n",
      "so:0.04804647214577567\n",
      "be:0.043474977303640576\n",
      "that:0.037753235905999265\n",
      "app:0.03650645791321497\n",
      "of:0.032674999272252266\n",
      "i:0.030922547279595262\n",
      "this:0.02942320640446362\n",
      "the:0.02936396860668698\n",
      "is:0.026028098842590743\n"
     ]
    }
   ],
   "source": [
    "doc = 0 #Check for our first review in our dataset\n",
    "print('Text 0:\\n{}\\n'.format(all_texts_preprocessed[0]))\n",
    "\n",
    "# Inspect tf-idf scores of document sorted by the similarity score\n",
    "for w,s in sorted(tfidf_model[corpus[doc]], reverse=True, key=lambda x: x[1]):\n",
    "    print('{}:{}'.format(dct[w], s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "# vectorize all documents with tf-idf\n",
    "tfidf_vectorization_csr = matutils.corpus2csc(tfidf_model[corpus], num_terms=len(dct))\n",
    "X_tfidf_vectorization = tfidf_vectorization_csr.T.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zimx86-x80yD"
   },
   "source": [
    "## **4.2 Word Embedding Vectorization (dense)**\n",
    "\n",
    "**Intro**\n",
    "\n",
    "Using TfIdf value for each token as the text representation will lead to the following problems:\n",
    "* high dimensional feature vector due to large size of vocabulary (V) or unseen words\n",
    "* lead to highly sparse vector (i.e. a lot of zeros)\n",
    "\n",
    "Here comes the question: how can we represent a word by encoding the co-occurrence information within context, but having a more dense representation (d << V)?\n",
    "\n",
    "**Distributional Hypothesis**\n",
    "* Words which are similar in meaning occur in similar contexts\n",
    "* Words that occur in the same contexts tend to have similar meanings\n",
    "\n",
    "Learn dense word representations (e.g. d=50, 100, 300, ...) by expoiting co-occurrence statistics with neighboring words. Each word is represented by a dense vector and the dimension of the semantic representation d is usually much\n",
    "smaller than the size of the vocabulary (d << V). All dimensions contain real-valued numbers (possibly normalized between −1 and 1).\n",
    "\n",
    "* **CBOW**: predict center word from neighboring words\n",
    "* **Skip-gram**: predict neighboring words from center word\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:720/format:webp/1*cuOmGT7NevP9oJFJfVpRKA.png\" width=800></img>\n",
    "</p>\n",
    "\n",
    "* **Glove**: leveraging global word to word co-occurance counts on the entire corpus\n",
    "* **FastText**: represent words by neighboring ngrams, capturing out-of-vocabulary (OOV) words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e6y7cWw_TX2"
   },
   "source": [
    "In order to use pretrained word embeddings, you just have to download the vectors from the authors web site. Often, it is just a text file with the following structure:\n",
    "\\<word> \\<tab> \\<vector> .\n",
    "\n",
    "Here, we can again make use of gensim, which already provides some [pretrained models](https://github.com/RaRe-Technologies/gensim-data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 434,
     "status": "ok",
     "timestamp": 1648124438086,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "fe4u6oNXBoEj"
   },
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41413,
     "status": "ok",
     "timestamp": 1648124480620,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "_YzaemW_BFlZ",
    "outputId": "e77e1723-6571-430b-eff0-56aa7fe2d11f"
   },
   "outputs": [],
   "source": [
    "# Load the glove model\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 217,
     "status": "ok",
     "timestamp": 1648124482731,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "WTjXfLN7KUvE",
    "outputId": "07e53935-5173-4d42-bb31-180c1005b378"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 400000\n"
     ]
    }
   ],
   "source": [
    "print('Vocab size: {}'.format(len(glove_model.index_to_key)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 227,
     "status": "ok",
     "timestamp": 1648124484689,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "NnC0r1qSCAD_",
    "outputId": "768bc8e0-dd1c-4462-eb7e-edaa20dcc286"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.3121e-02,  7.1226e-01, -2.2566e-01,  1.2239e-01,  7.2298e-01,\n",
       "        5.5787e-01, -6.4484e-03, -7.0540e-01, -3.2206e-01,  1.2796e-01,\n",
       "       -7.7531e-02,  5.5125e-02,  3.8379e-02,  9.5295e-01,  4.3992e-02,\n",
       "        4.5025e-01, -1.1435e-01, -1.2781e-03, -3.2919e-01, -1.8721e+00,\n",
       "        1.0702e+00, -1.1634e-01, -6.2644e-01, -6.1095e-01, -4.1408e-01,\n",
       "       -5.5053e-01, -7.0974e-01,  1.4821e+00,  5.3134e-01, -5.3206e-01,\n",
       "        2.1137e+00, -1.0961e+00,  4.8239e-01, -1.6375e-01, -6.2490e-01,\n",
       "        7.2829e-01,  5.1436e-01,  8.4971e-01,  1.0365e+00,  7.4664e-02,\n",
       "        1.0125e+00,  4.7441e-01,  4.2126e-01, -4.9965e-02,  5.3149e-01,\n",
       "        1.2321e+00, -2.7239e-01, -1.4639e+00, -3.3859e-01, -9.7446e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model[\"glass\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 330,
     "status": "ok",
     "timestamp": 1648124487913,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "vJlha75XB-Ls",
    "outputId": "07896f2f-f09b-4d6c-e724-0c0d41ac5c6f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('plastic', 0.79425048828125),\n",
       " ('metal', 0.7708716988563538),\n",
       " ('walls', 0.7700635194778442),\n",
       " ('marble', 0.7638523578643799),\n",
       " ('wood', 0.7624280452728271),\n",
       " ('ceramic', 0.7602593302726746),\n",
       " ('pieces', 0.7589112520217896),\n",
       " ('stained', 0.7528817653656006),\n",
       " ('tile', 0.748193621635437),\n",
       " ('furniture', 0.7463858723640442)]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_model.most_similar(\"glass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "spAPjv2F_Hax"
   },
   "source": [
    "We can transform, i.e., vectorize, our text by looking up the pretrained embedding for each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 220,
     "status": "ok",
     "timestamp": 1648124498750,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "Mk5fRWnqCs_Y",
    "outputId": "c8a7a028-c108-4493-ea87-7dfd469fcca1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([-0.25676 ,  0.8549  ,  1.1003  ,  0.95363 ,  0.36585 , -1.3029  ,\n",
       "         1.0754  , -0.18461 , -0.67674 ,  0.37637 , -0.029637,  0.51698 ,\n",
       "        -0.19248 , -0.41863 , -0.71144 ,  0.12564 , -0.42965 ,  0.61456 ,\n",
       "         0.41819 ,  0.27606 , -0.48635 , -0.32585 ,  0.67748 ,  0.15916 ,\n",
       "         0.35051 , -0.29393 , -0.80439 , -0.15939 ,  0.012475, -0.58404 ,\n",
       "         2.1353  , -0.1547  , -0.5739  ,  1.4522  ,  0.6124  , -0.68752 ,\n",
       "         1.2839  , -0.54631 , -0.35737 ,  0.57323 ,  0.3546  , -0.37465 ,\n",
       "        -0.74628 , -0.074561, -0.48471 ,  0.067343, -0.039338, -0.22177 ,\n",
       "         0.099708,  0.55553 ], dtype=float32),\n",
       " array([-3.5559e-01,  1.2386e+00,  1.4348e+00,  1.0447e+00,  1.0335e+00,\n",
       "         1.0445e-01,  2.7760e-01, -1.2675e+00, -9.5788e-01, -5.9603e-01,\n",
       "        -1.8280e-01, -1.8247e-02, -9.3553e-01, -4.9706e-01,  8.5845e-03,\n",
       "         8.6971e-01, -3.5250e-01, -1.7587e-01, -6.7203e-01, -4.9188e-01,\n",
       "         4.9252e-05,  1.9287e-01,  1.2353e+00,  7.1928e-01, -3.7632e-01,\n",
       "         6.0005e-01, -2.1616e-01,  2.6368e-01,  3.7403e-01,  8.5283e-01,\n",
       "         2.2797e+00, -2.9896e-01,  1.1625e-01,  7.9111e-01,  5.7041e-01,\n",
       "        -1.6061e+00, -2.0628e-02, -1.1018e+00,  1.2016e+00,  2.8124e-01,\n",
       "         4.4690e-01, -5.8974e-01, -3.1770e-01,  4.9358e-01, -3.8290e-01,\n",
       "         3.3543e-01,  7.0644e-01,  1.6022e-01,  1.1328e+00,  6.9397e-01],\n",
       "       dtype=float32),\n",
       " array([ 0.07487 ,  0.7268  ,  0.60202 ,  0.67041 , -0.4931  ,  0.93332 ,\n",
       "         0.56347 , -0.6014  , -0.24315 , -0.65394 , -1.0592  , -0.022451,\n",
       "        -0.18103 , -0.60974 , -0.92036 , -0.61579 ,  0.18427 , -0.20126 ,\n",
       "        -0.17767 , -0.73492 , -1.0046  , -0.93514 ,  0.81081 ,  0.26421 ,\n",
       "        -0.81027 , -1.1659  ,  0.22443 , -0.11386 , -0.65991 , -0.22088 ,\n",
       "         1.9264  , -0.18013 , -0.67806 ,  0.097938,  0.7236  , -1.463   ,\n",
       "         0.672   ,  0.19596 ,  0.64471 , -0.1105  ,  0.61934 ,  0.87038 ,\n",
       "        -0.36938 , -0.49484 , -0.054509,  1.2095  ,  0.14817 , -0.3651  ,\n",
       "        -0.34448 ,  2.5774  ], dtype=float32),\n",
       " array([ 0.21705 ,  0.46515 , -0.46757 ,  0.10082 ,  1.0135  ,  0.74845 ,\n",
       "        -0.53104 , -0.26256 ,  0.16812 ,  0.13182 , -0.24909 , -0.44185 ,\n",
       "        -0.21739 ,  0.51004 ,  0.13448 , -0.43141 , -0.03123 ,  0.20674 ,\n",
       "        -0.78138 , -0.20148 , -0.097401,  0.16088 , -0.61836 , -0.18504 ,\n",
       "        -0.12461 , -2.2526  , -0.22321 ,  0.5043  ,  0.32257 ,  0.15313 ,\n",
       "         3.9636  , -0.71365 , -0.67012 ,  0.28388 ,  0.21738 ,  0.14433 ,\n",
       "         0.25926 ,  0.23434 ,  0.4274  , -0.44451 ,  0.13813 ,  0.36973 ,\n",
       "        -0.64289 ,  0.024142, -0.039315, -0.26037 ,  0.12017 , -0.043782,\n",
       "         0.41013 ,  0.1796  ], dtype=float32),\n",
       " array([-0.37915 ,  0.61848 ,  0.9593  ,  0.90403 ,  0.36806 ,  0.022972,\n",
       "         0.16795 , -1.5309  , -0.060533, -0.25    ,  0.15031 ,  0.31967 ,\n",
       "        -0.68914 , -0.78626 , -0.015825,  0.50531 , -0.8473  , -0.12353 ,\n",
       "         0.078526, -0.96024 , -0.54313 , -0.33516 ,  0.38932 ,  0.19461 ,\n",
       "        -1.1688  , -0.86608 , -0.39178 ,  0.24183 ,  0.32862 , -0.78755 ,\n",
       "         2.4884  ,  0.71015 , -0.53114 ,  0.89593 , -0.23053 , -0.82023 ,\n",
       "         0.34425 , -0.96866 , -0.15143 , -0.44912 ,  0.89513 , -0.01659 ,\n",
       "        -0.2749  ,  0.27948 ,  0.77936 , -0.31944 ,  0.16756 , -0.62515 ,\n",
       "         0.053323,  0.62023 ], dtype=float32)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_texts_tokenized_embedded = [[glove_model[w] for w in t if w in glove_model]  for t in all_texts_preprocessed]\n",
    "all_texts_tokenized_embedded[0][0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MkXwaXveDNXv"
   },
   "source": [
    "Now, we have assigned a vector to each word. But how to get a sentence representation? For instance, we can just average the individual word vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12963,
     "status": "ok",
     "timestamp": 1648124522962,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "dM9FmubeDdpR",
    "outputId": "f4873591-ae5f-4201-84e1-5103c47f3035"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\numpy\\lib\\function_base.py:518: RuntimeWarning: Mean of empty slice.\n",
      "  avg = a.mean(axis, **keepdims_kw)\n",
      "C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:190: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "all_texts_tokenized_embedded = [[glove_model[w].astype(float) for w in t if w in glove_model] for t in all_texts_preprocessed]\n",
    "X_all_texts_tokenized_embedded_averaged = [np.average(t, axis=0) for t in all_texts_tokenized_embedded]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1648124528644,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "N-b3qjtr8KQQ",
    "outputId": "0f91759f-bfe4-4029-9a9d-ed272ec169c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "یاشار\n",
      "شكرا\n",
      "♡♡\n",
      "عملي جدآ\n",
      "提醒方式可設定成全螢幕鬧鐘模式，不會錯過\n"
     ]
    }
   ],
   "source": [
    "# Further find out that there are some texts which are not representative by using glove\n",
    "for i, k in enumerate(X_all_texts_tokenized_embedded_averaged):\n",
    "    if np.isnan(k).any():\n",
    "        print(all_texts_preprocessed[i])\n",
    "    \n",
    "# For these cases we can simply assign a random word vector from gensim.model\n",
    "for i, k in enumerate(X_all_texts_tokenized_embedded_averaged):\n",
    "    if np.isnan(k).any():\n",
    "        X_all_texts_tokenized_embedded_averaged[i]=glove_model[random.choice(glove_model.index_to_key)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 210,
     "status": "ok",
     "timestamp": 1648124536086,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "uMWBV6nvBHrM",
    "outputId": "ac324da1-ac67-47a0-dbf1-f1e20657ea6a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15746, 50)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all_texts_tokenized_embedded_averaged = np.array(X_all_texts_tokenized_embedded_averaged)\n",
    "X_all_texts_tokenized_embedded_averaged.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LoIHJavMrKWR"
   },
   "source": [
    "# **5. ML Framework: scikit-learn (sklearn)**\n",
    "\n",
    "[Scikit-learn](https://scikit-learn.org/stable/) (formerly scikits.learn and also known as sklearn) is a free software machine learning library for the Python programming language. It features various classification, regression and clustering algorithms including support vector machines, random forests, gradient boosting, k-means...etc, and is designed to interoperate with the Python numerical and scientific libraries NumPy and SciPy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sR6nzxqbrvnm"
   },
   "source": [
    "## **5.1 Classification**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "ok",
     "timestamp": 1648125095211,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "44WsA9M0yk_Q",
    "outputId": "edc6c09c-da38-4cb3-cfa6-be5a80387800"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15746, 11042)\n",
      "(15746,)\n"
     ]
    }
   ],
   "source": [
    "# Now our input can either be X_tfidf_vectorization or X_all_texts_tokenized_embedded_averaged\n",
    "# Case1: X = X_tfidf_vectorization\n",
    "\n",
    "X = X_tfidf_vectorization\n",
    "print(X.shape)\n",
    "Y = np.array(list(df['sentiment']))\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1424,
     "status": "ok",
     "timestamp": 1648125098001,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "OFYh3Et6zaTh",
    "outputId": "ca984ec6-83ed-402e-e0e0-c7179210b71a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12596 1575 1575\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# split train, val, test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=RANDOM_SEED, shuffle=True)\n",
    "print(len(X_train), len(X_val), len(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 105548,
     "status": "ok",
     "timestamp": 1648128718366,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "RnXyJuaV1Ual",
    "outputId": "fd17727a-212b-4360-cc65-1bc315c91dff"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Usuario\\Desktop\\QFM\\S2\\webmining\\2-Part 2 Lab SA\\2-Sentiment analysis Google Reviews Dataset.ipynb Cell 67\u001b[0m in \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/QFM/S2/webmining/2-Part%202%20Lab%20SA/2-Sentiment%20analysis%20Google%20Reviews%20Dataset.ipynb#Y123sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# SGDClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/QFM/S2/webmining/2-Part%202%20Lab%20SA/2-Sentiment%20analysis%20Google%20Reviews%20Dataset.ipynb#Y123sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinear_model\u001b[39;00m \u001b[39mimport\u001b[39;00m SGDClassifier\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/QFM/S2/webmining/2-Part%202%20Lab%20SA/2-Sentiment%20analysis%20Google%20Reviews%20Dataset.ipynb#Y123sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m precision_recall_fscore_support\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Usuario/Desktop/QFM/S2/webmining/2-Part%202%20Lab%20SA/2-Sentiment%20analysis%20Google%20Reviews%20Dataset.ipynb#Y123sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "# SGDClassifier: https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize list of classifiers\n",
    "classifiers_to_test = [\n",
    "    ('SGD trained for 200 iters', SGDClassifier(max_iter = 200, random_state=RANDOM_SEED)),\n",
    "    ('SGD trained for 200 iters, lr=adaptive, eta0=0.1, class_weights=balanced', SGDClassifier(max_iter = 200, learning_rate = 'adaptive', eta0=0.1, class_weight='balanced', random_state=RANDOM_SEED)),\n",
    "    ('SGD trained for 200 iters, lr=adaptive, eta0=0.1, class_weights=balanced, loss=log',SGDClassifier(max_iter = 200, learning_rate = 'adaptive', eta0=0.1, class_weight='balanced', loss='log_loss', random_state=RANDOM_SEED))\n",
    "]\n",
    "\n",
    "# train evaluate classifiers\n",
    "for name, classifier in classifiers_to_test:\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict Class\n",
    "    y_pred = classifier.predict(X_val)\n",
    "\n",
    "    # Accuracy \n",
    "    print('Classifier: {}'.format(name))\n",
    "    print('Accuracy: {}'.format(accuracy_score(y_val, y_pred)*100))\n",
    "    P, R, F1, _ = precision_recall_fscore_support(y_val, y_pred, average='macro')\n",
    "    print('P: {}, R:{}, F1: {}'.format(P*100, R*100, F1*100))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best classifier on test set\n",
    "y_pred = classifiers_to_test[1][1].predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "print('Classifier: {}'.format(name))\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))\n",
    "P, R, F1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print('P: {}, R:{}, F1: {}'.format(P, R, F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case2: X = X_all_texts_tokenized_embedded_averaged\n",
    "# If our input is the average embedding for the text tokens\n",
    "\n",
    "X = X_all_texts_tokenized_embedded_averaged\n",
    "Y = np.array(list(df['sentiment']))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=RANDOM_SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1944,
     "status": "ok",
     "timestamp": 1648125004006,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "Tx9cR-VB1pPq",
    "outputId": "6e5e41b2-c3f3-4e0c-ca9f-a96fa531bb19"
   },
   "outputs": [],
   "source": [
    "# Initialize list of classifiers\n",
    "classifiers_to_test = [\n",
    "    ('SGD trained for 200 iters', \n",
    "         SGDClassifier(max_iter = 200, random_state=RANDOM_SEED)),\n",
    "    ('SGD trained for 200 iters, lr=adaptive, eta0=0.1, class_weights=balanced', \n",
    "         SGDClassifier(max_iter = 200, learning_rate = 'adaptive', eta0=0.1, class_weight='balanced', random_state=RANDOM_SEED)),\n",
    "    ('SGD trained for 200 iters, lr=adaptive, eta0=0.1, class_weights=balanced, loss=log',\n",
    "        SGDClassifier(max_iter = 200, learning_rate = 'adaptive', eta0=0.1, class_weight='balanced', loss='log_loss', random_state=RANDOM_SEED))\n",
    "]\n",
    "\n",
    "# train evaluate classifiers\n",
    "for name, classifier in classifiers_to_test:\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict Class\n",
    "    y_pred = classifier.predict(X_val)\n",
    "\n",
    "    # Accuracy \n",
    "    print('Classifier: {}'.format(name))\n",
    "    print('Accuracy: {}'.format(accuracy_score(y_val, y_pred)))\n",
    "    P, R, F1, _ = precision_recall_fscore_support(y_val, y_pred, average='macro')\n",
    "    print('P: {}, R:{}, F1: {}'.format(P, R, F1))\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate best classifier on test set\n",
    "y_pred = classifiers_to_test[2][1].predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "print('Classifier: {}'.format(name))\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))\n",
    "P, R, F1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print('P: {}, R:{}, F1: {}'.format(P, R, F1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Cross-Validation for hyperparameter tuning.\n",
    "\n",
    "**Running the following cell is computationally expensive and will take some time!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Cross Validation to tune hyperparameters\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = X_tfidf_vectorization\n",
    "Y = np.array(list(df['sentiment']))\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=RANDOM_SEED, shuffle=True)\n",
    "\n",
    "# create an classifier\n",
    "classifier = SGDClassifier(max_iter=200, learning_rate = 'adaptive', class_weight='balanced', random_state=RANDOM_SEED)\n",
    "\n",
    "# specify the parameter grid\n",
    "parameters = {\n",
    "    'eta0': [0.1, 0.001, 0.0001]\n",
    "}\n",
    "\n",
    "# specify the cross validation\n",
    "stratified_5_fold_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# create the grid search instance\n",
    "grid_search_estimator = GridSearchCV(classifier, parameters, scoring='accuracy', cv=stratified_5_fold_cv, return_train_score=False)\n",
    "\n",
    "# run the grid search\n",
    "grid_search_estimator.fit(X_train,y_train)\n",
    "\n",
    "# print the results of all hyper-parameter combinations\n",
    "results = pd.DataFrame(grid_search_estimator.cv_results_)\n",
    "display(results)\n",
    "    \n",
    "# print the best parameter setting\n",
    "print(\"best score is {} with params {}\".format(grid_search_estimator.best_score_, grid_search_estimator.best_params_))\n",
    "\n",
    "# Predict Class\n",
    "y_pred = grid_search_estimator.predict(X_test)\n",
    "\n",
    "# Accuracy \n",
    "print('Classifier: {}'.format(name))\n",
    "print('Accuracy: {}'.format(accuracy_score(y_test, y_pred)))\n",
    "P, R, F1, _ = precision_recall_fscore_support(y_test, y_pred, average='macro')\n",
    "print('P: {}, R:{}, F1: {}'.format(P, R, F1))\n",
    "print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 565,
     "status": "ok",
     "timestamp": 1648125203756,
     "user": {
      "displayName": "Chia-Chien Hung",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "04879890229820011815"
     },
     "user_tz": -60
    },
    "id": "zTFRXMLTKm0p",
    "outputId": "233709ff-f74e-4c26-930f-d54f1f4e0082",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### How about writing a prediction function for inputing an unseen text, and check the predicted result.\n",
    "x_new = \"I like this app. It's super useful!😄\"\n",
    "x_new_preprocessed = ' '.join(nltk.word_tokenize(emoji.demojize(x_new.lower(), language='en')))\n",
    "\n",
    "# Vectorize with gensim\n",
    "x_new_preprocessed_corpus = dct.doc2bow(simple_preprocess(x_new_preprocessed))\n",
    "x_new_preprocessed_tfidf = tfidf_model[x_new_preprocessed_corpus]\n",
    "\n",
    "# Convert vectorization to process in sklearn\n",
    "x_new_preprocessed_tfidf_csr = matutils.corpus2csc([x_new_preprocessed_tfidf], num_terms=len(dct))\n",
    "x_new_preprocessed_tfidf_vectorization = x_new_preprocessed_tfidf_csr.T.toarray()\n",
    "print(x_new_preprocessed_tfidf_vectorization)\n",
    "\n",
    "mapping_class = {0: \"negative\", 1: \"neutral\", 2: \"positive\"}\n",
    "print('Predicted Class: {}'.format(mapping_class[grid_search_estimator.predict(x_new_preprocessed_tfidf_vectorization)[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xw3MbJ5uJrHY"
   },
   "source": [
    "### More advanced: try fasttext embedding instead of glove to deal with OOV words\n",
    "https://github.com/facebookresearch/fastText/\n",
    "1. !pip install fasttext\n",
    "2. Download the model from their website\n",
    "\n",
    "> model = fasttext.load_model(\"model_filename.bin\")\n",
    "\n",
    "> model = fasttext.train_unsupervised('data.txt', model='skipgram')\n",
    "\n",
    "3. try other classifiers from scikit-learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wheel in c:\\users\\usuario\\anaconda3\\lib\\site-packages (0.37.1)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Installing collected packages: wheel\n",
      "  Attempting uninstall: wheel\n",
      "    Found existing installation: wheel 0.37.1\n",
      "    Uninstalling wheel-0.37.1:\n",
      "      Successfully uninstalled wheel-0.37.1\n",
      "Successfully installed wheel-0.40.0\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade wheel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting fasttext\n",
      "  Using cached fasttext-0.9.2.tar.gz (68 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from fasttext) (2.10.4)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from fasttext) (67.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\usuario\\anaconda3\\lib\\site-packages (from fasttext) (1.23.5)\n",
      "Building wheels for collected packages: fasttext\n",
      "  Building wheel for fasttext (setup.py): started\n",
      "  Building wheel for fasttext (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for fasttext\n",
      "Failed to build fasttext\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  python setup.py bdist_wheel did not run successfully.\n",
      "  exit code: 1\n",
      "  \n",
      "  [43 lines of output]\n",
      "  C:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\setuptools\\dist.py:755: SetuptoolsDeprecationWarning: Invalid dash-separated options\n",
      "  !!\n",
      "  \n",
      "          ********************************************************************************\n",
      "          Usage of dash-separated 'description-file' will not be supported in future\n",
      "          versions. Please use the underscore name 'description_file' instead.\n",
      "  \n",
      "          By 2023-Sep-26, you need to update your project and remove deprecated calls\n",
      "          or your builds will no longer be supported.\n",
      "  \n",
      "          See https://setuptools.pypa.io/en/latest/userguide/declarative_config.html for details.\n",
      "          ********************************************************************************\n",
      "  \n",
      "  !!\n",
      "    opt = self.warn_dash_deprecation(opt, section)\n",
      "  running bdist_wheel\n",
      "  running build\n",
      "  running build_py\n",
      "  creating build\n",
      "  creating build\\lib.win-amd64-cpython-39\n",
      "  creating build\\lib.win-amd64-cpython-39\\fasttext\n",
      "  copying python\\fasttext_module\\fasttext\\FastText.py -> build\\lib.win-amd64-cpython-39\\fasttext\n",
      "  copying python\\fasttext_module\\fasttext\\__init__.py -> build\\lib.win-amd64-cpython-39\\fasttext\n",
      "  creating build\\lib.win-amd64-cpython-39\\fasttext\\util\n",
      "  copying python\\fasttext_module\\fasttext\\util\\util.py -> build\\lib.win-amd64-cpython-39\\fasttext\\util\n",
      "  copying python\\fasttext_module\\fasttext\\util\\__init__.py -> build\\lib.win-amd64-cpython-39\\fasttext\\util\n",
      "  creating build\\lib.win-amd64-cpython-39\\fasttext\\tests\n",
      "  copying python\\fasttext_module\\fasttext\\tests\\test_configurations.py -> build\\lib.win-amd64-cpython-39\\fasttext\\tests\n",
      "  copying python\\fasttext_module\\fasttext\\tests\\test_script.py -> build\\lib.win-amd64-cpython-39\\fasttext\\tests\n",
      "  copying python\\fasttext_module\\fasttext\\tests\\__init__.py -> build\\lib.win-amd64-cpython-39\\fasttext\\tests\n",
      "  running build_ext\n",
      "  building 'fasttext_pybind' extension\n",
      "  creating build\\temp.win-amd64-cpython-39\n",
      "  creating build\\temp.win-amd64-cpython-39\\Release\n",
      "  creating build\\temp.win-amd64-cpython-39\\Release\\python\n",
      "  creating build\\temp.win-amd64-cpython-39\\Release\\python\\fasttext_module\n",
      "  creating build\\temp.win-amd64-cpython-39\\Release\\python\\fasttext_module\\fasttext\n",
      "  creating build\\temp.win-amd64-cpython-39\\Release\\python\\fasttext_module\\fasttext\\pybind\n",
      "  creating build\\temp.win-amd64-cpython-39\\Release\\src\n",
      "  \"C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.34.31933\\bin\\HostX86\\x64\\cl.exe\" /c /nologo /O2 /W3 /GL /DNDEBUG /MD -IC:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\pybind11\\include -IC:\\Users\\Usuario\\anaconda3\\lib\\site-packages\\pybind11\\include -Isrc -IC:\\Users\\Usuario\\anaconda3\\include -IC:\\Users\\Usuario\\anaconda3\\Include \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.34.31933\\include\" \"-IC:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Auxiliary\\VS\\include\" \"-IC:\\Program Files (x86)\\Windows Kits\\NETFXSDK\\4.8\\include\\um\" /EHsc /Tppython/fasttext_module/fasttext/pybind/fasttext_pybind.cc /Fobuild\\temp.win-amd64-cpython-39\\Release\\python/fasttext_module/fasttext/pybind/fasttext_pybind.obj /EHsc /DVERSION_INFO=\\\\\\\"0.9.2\\\\\\\"\n",
      "  fasttext_pybind.cc\n",
      "  C:\\Program Files (x86)\\Microsoft Visual Studio\\2022\\BuildTools\\VC\\Tools\\MSVC\\14.34.31933\\include\\yvals.h(17): fatal error C1083: Cannot open include file: 'crtdbg.h': No such file or directory\n",
      "  error: command 'C:\\\\Program Files (x86)\\\\Microsoft Visual Studio\\\\2022\\\\BuildTools\\\\VC\\\\Tools\\\\MSVC\\\\14.34.31933\\\\bin\\\\HostX86\\\\x64\\\\cl.exe' failed with exit code 2\n",
      "  [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for fasttext\n",
      "ERROR: Could not build wheels for fasttext, which is required to install pyproject.toml-based projects\n"
     ]
    }
   ],
   "source": [
    "pip install fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'fasttext'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mfasttext\u001b[39;00m\n\u001b[0;32m      2\u001b[0m fasttext\u001b[38;5;241m.\u001b[39mutil\u001b[38;5;241m.\u001b[39mdownload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men\u001b[39m\u001b[38;5;124m'\u001b[39m, if_exists\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# English\u001b[39;00m\n\u001b[0;32m      3\u001b[0m ft \u001b[38;5;241m=\u001b[39m fasttext\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcc.en.300.bin\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fasttext'"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "fasttext.util.download_model('en', if_exists='ignore')  # English\n",
    "ft = fasttext.load_model('cc.en.300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNc5p2ULPDMe4ho9iKUVSQE",
   "collapsed_sections": [],
   "name": "Lab5_web content mining_update.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
